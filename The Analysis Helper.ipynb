{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Analysis Helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents:\n",
    "    1. Obtaining the data\n",
    "        a. Imports\n",
    "        b. Reading the file\n",
    "        c. Feature extraction\n",
    "        d. Binarification\n",
    "        e. Missing values\n",
    "        f. Scaling\n",
    "        g. Outliers\n",
    "    2. Exploring the rankings\n",
    "        a. Ranking table\n",
    "        b. Object popularity\n",
    "        c. Clustering\n",
    "        d. Cluster popularity\n",
    "    3. Prediction model\n",
    "        a. Initial run\n",
    "        b. Feature selection\n",
    "        c. Hyperparameter tuning\n",
    "        d. Model significance\n",
    "        e. Final run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtaining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. a. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to import all the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# our modules\n",
    "%run ./obtainingTheData.ipynb\n",
    "%run ./exploringTheRankings.ipynb\n",
    "%run ./predictionModel.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. b. Reading the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the file path. The function will then read from this file.\n",
    "\n",
    "Example:\n",
    "- file = 'filename.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'filename.csv'\n",
    "\n",
    "str_data, flt_data = csvGet(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check: Set the column title and row number to check whether the cell value is equivalent to the value in the csv.\n",
    "\n",
    "Example: \n",
    "- column_title = 'my_column'\n",
    "- row_number = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_title = 'my_column'\n",
    "row_number = 5\n",
    "\n",
    "sanityCheck1(str_data, column_title, row_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. c. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this to filter the rows. Set the column and category to only retain data from that category.\n",
    "\n",
    "Example - Only retain data where the 'Type' is 'One':\n",
    "- column = 'Type'\n",
    "- category = 'One'\n",
    "\n",
    "Example - use this setting to retain all data:\n",
    "- column = None\n",
    "- category = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'Type'\n",
    "category = 'One'\n",
    "    \n",
    "filtered_flt_data, filtered_str_data = retainRows(flt_data, str_data, column, category)\n",
    "\n",
    "sanityCheck2(flt_data, filtered_flt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get specific useful columns - the y labels, the stimulus labels, and the subject IDs. Set the labels for the csv columns matching the desired variables, and the function will retrieve them.\n",
    "\n",
    "Example:\n",
    "- y_label = 'Ranking_Stimuli'\n",
    "- stimulus_label = 'Stimulus'\n",
    "- subjectID_label = 'SubID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = 'Ranking_Stimuli'\n",
    "stimulus_label = 'Stimulus'\n",
    "subjectID_label = 'SubID'\n",
    "\n",
    "y_col, stimulus_col, subID_col, object_list, subID_list = getColumns(filtered_flt_data, filtered_str_data, y_label, stimulus_label, subjectID_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the X-variables (features): Set the list of features you would like to include as X-values in the analysis. The function will then create the X-matrix.\n",
    "\n",
    "Example:\n",
    "- feature_list = ['Feature_one', 'Featue_two', 'Feature_three']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = ['Feature_one', 'Featue_two', 'Feature_three']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = getFeatures(filtered_flt_data, feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. d. Binarification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell will create a binary version of the y data (a column with 0s and 1s). This is done according to the median - a value above the median will become 1, otherwise it will become 0. Set the median type to 'subject' or 'overall', depending on which median you would like to use.\n",
    "\n",
    "Example:\n",
    "- median = 'subject'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = 'subject'\n",
    "\n",
    "binary_y_col = binarify(y_col, subID_col, median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data check: it is important for the y-labels to be balanced (a similar percentage of 0s and 1s). The next cell will report the precentage of 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCheck(binary_y_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. e. Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to replace missing values in the data with the average value of the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_X = replaceMissingValues(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. f. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to scale each feature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = scale(replaced_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. g. Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the fraction of data you are considering removing. You can play with this number to mark different amounts.\n",
    "\n",
    "Example: remove 1% of the data:\n",
    "- outlier_fraction = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_fraction = 0.01\n",
    "\n",
    "sanityCheck3(scaled_X, outlier_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run this cell to find and visualize the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_indices = displayOutliers(scaled_X, outlier_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, run this cell to remove the outliers from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_X, processed_bi_y, processed_y, processed_stim_col, processed_subID_col = removeOutliers(\n",
    "    outliers_indices, scaled_X, binary_y_col, y_col, stimulus_col, subID_col)\n",
    "\n",
    "sanityCheck2(scaled_X, processed_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a. Ranking table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell creates a table (\"dataframe\") where the rows represent subjects, and the columns represent stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_dataframe = createDataframe(\n",
    "    processed_subID_col, processed_stim_col, processed_bi_y, object_list, subID_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A peek of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell enables you to get the ranking a certain subject gave a certain object from the table.\n",
    "\n",
    "Example: to view the ranking (0 or 1) subject 1 gave the 'Stimulus_one' stimulus:\n",
    "- subject_ID = 1\n",
    "- object_name = 'Stimulus_one'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_ID = 1\n",
    "object_name = 'Stimulus_one'\n",
    "\n",
    "getRanking(ranking_dataframe, subject_ID, object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. b. Object popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list of the objects from least favorite to most favorite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favoriteObjects(ranking_dataframe, object_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. c. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the desired number of clusters\n",
    "\n",
    "Example:\n",
    "- num_clusters = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform clustering and display the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels, t_rankings = getClusters(ranking_dataframe, num_clusters, object_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the clusters on a PCA graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = visualizeClusters(cluster_labels, t_rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. d. Cluster popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to make sure the popular and unpopular objects are balanced between the clusters.\n",
    "This will be done in two ways. An object's popularity will be measured as the amount of subjects who ranked it 1.\n",
    "The minimum possible score is zero, while the maximum possible score is the number of subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color the objects based on popularity, to make sure that it is evenly scattered in the diffefent clusters. A lighter color means the object is more popluar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularities = visualizePopularity(pca_components, ranking_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the mean popularity for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayPopularity(popularities, num_clusters, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. a. Initial run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform 10-fold cross-validation on the model, then display evaluations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df, predictions = svmModel(processed_X, processed_bi_y, SVC(kernel='rbf', C=1, gamma=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check: during the cross-validation run, the model should have made a roughly balanced amount of '1' and '0' predictions. The next cell will check how many such predictions were made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanityCheck4(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. b. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A reminder: this is the current feature list used for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now moving on to select a subset of features that yields good performance. Set the desired number of features. If set to None, up to 1/2 of the features will be selected. If set to a number, that is the max number of selected features. Note that the number affects the runtime.\n",
    "\n",
    "Examples:\n",
    "- n_features = None\n",
    "- n_features = 3 (with 3, the function will run for about 1 minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential feature selection: this function will try to find a subset of features that yields good performance from the model. It will take a bit of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection, selection_mask = featureSelect(processed_X, processed_bi_y, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The list of selected features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An evaluation of the model with the new feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df, predictions, new_feature_X = evaluateFeatureSet(processed_X, processed_bi_y, selection_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check: again, during the cross-validation run, the model should have made a roughly balanced amount of '1' and '0' predictions. Display this ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanityCheck4(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, you muse decide whether you would like to use the previous features list, or the new subset.\n",
    "\n",
    "To use the old feature set:\n",
    "- final_X = processed_X\n",
    "\n",
    "To use the new feature set:\n",
    "- final_X = new_feature_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X = new_feature_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. b. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next cell will perform a 'grid search' - it will try out the RBF SVM model with different values for the c and gamma parameters, and report which ones yield the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridSearch(final_X, processed_bi_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the final model's parameters to your desire. We suggest basing the selection on these results.\n",
    "\n",
    "Example:\n",
    "- new_C = 1\n",
    "- new_gamma = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_C = 1\n",
    "new_gamma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An evaluation of the model with the new parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df, predictions = svmModel(final_X, processed_bi_y, SVC(kernel='rbf', C=new_C, gamma=new_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check: again, during the cross-validation run, the model should have made a roughly balanced amount of '1' and '0' predictions. Display this ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanityCheck4(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. d. Model significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the number of runs for a permutation test. This will affect runtime.\n",
    "\n",
    "Example:\n",
    "- n_permutations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_permutations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next cell will perform a permutation test to check the model's p-value. This should take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = permutationTest(n_permutations, final_X, processed_bi_y, SVC(kernel='rbf', C=new_C, gamma=new_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. e. Final run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that the features and parameters have been selected, it's time to create the final model, and train it on all available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = finalModel(final_X, processed_bi_y, new_C, new_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the model's performance on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.score(final_X, processed_bi_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check: again, the model should have made a roughly balanced amount of '1' and '0' predictions. Display this ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanityCheck4(svm_model.predict(final_X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
