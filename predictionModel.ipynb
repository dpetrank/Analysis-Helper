{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svmModel(X, y, svm):\n",
    "    \n",
    "    # create the k-fold object\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    \n",
    "    # arrays to collect data fold scores\n",
    "    accuracies = np.array([])\n",
    "    precisions = np.array([])\n",
    "    recalls = np.array([])\n",
    "    f1s = np.array([])\n",
    "    specificities = np.array([])\n",
    "    aucs = np.array([])\n",
    "    predictions = np.array([])\n",
    "    \n",
    "    # iterate over the 10 folds\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        # split data\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # fit the model\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_predicted = svm.predict(X_test)\n",
    "        \n",
    "        predictions = np.append(predictions, y_predicted)\n",
    "        \n",
    "        # collect the fold's scores\n",
    "        y_scores = svm.decision_function(X_test)\n",
    "\n",
    "        accuracies, precisions, recalls, f1s, specificities, aucs = appendScores(\n",
    "            y_test, y_predicted, y_scores, accuracies, precisions, recalls, f1s, specificities, aucs)\n",
    "        \n",
    "    # record the scores for this model\n",
    "    summary_df = getScores(accuracies, precisions, recalls,f1s, specificities, aucs)\n",
    "    \n",
    "    return summary_df, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendScores(y_test, y_predicted, y_scores, accuracies, precisions, recalls, f1s, specificities, aucs):\n",
    "     \n",
    "    current_accuracy, current_precision, current_recall, current_f1, current_specificity, current_auc = \\\n",
    "    calculateScores(y_test, y_predicted, y_scores)\n",
    "    \n",
    "    # append the new scores \n",
    "    accuracies = np.append(accuracies, current_accuracy)\n",
    "    precisions = np.append(precisions, current_precision)\n",
    "    recalls = np.append(recalls, current_recall)\n",
    "    f1s = np.append(f1s, current_f1)\n",
    "    specificities = np.append(specificities, current_specificity)\n",
    "    aucs = np.append(aucs, current_auc)\n",
    "    \n",
    "    return accuracies, precisions, recalls, f1s, specificities, aucs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateScores(y_true, y_predicted, y_scores):\n",
    "    \n",
    "    # Get true-negative, false-positive, false-negative and true-positive from confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_predicted).ravel()\n",
    "\n",
    "    # get the accuracy score\n",
    "    accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "\n",
    "    # get the percision score\n",
    "    if tp+fp == 0:\n",
    "        precision = 0\n",
    "    else:        \n",
    "        precision = tp/(tp+fp)\n",
    "\n",
    "    # get the recall score\n",
    "    if tp+fn == 0:\n",
    "        recall = 0\n",
    "    else:        \n",
    "        recall = tp/(tp+fn)\n",
    "    \n",
    "    # get the f1 score\n",
    "    if recall+precision == 0:\n",
    "        f1 = 0\n",
    "    else:    \n",
    "        f1 = (2*precision*recall)/(precision+recall)\n",
    "\n",
    "    # get the specificity score\n",
    "    if tn+fp == 0:\n",
    "        specificity = 0\n",
    "    else:\n",
    "        specificity = tn/(tn+fp)\n",
    "     \n",
    "    # calculate the auc\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, specificity, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScores(accuracies, precisions, recalls, f1s, specificities, aucs):\n",
    "    \n",
    "    # calculate mean & std for each evaluator\n",
    "    mean_summary = [np.mean(accuracies), np.mean(precisions), np.mean(recalls),\n",
    "                    np.mean(f1s), np.mean(specificities), np.mean(aucs)]\n",
    "    \n",
    "    std_summary = [np.std(accuracies), np.std(precisions), np.std(recalls),\n",
    "                   np.std(f1s), np.std(specificities), np.std(aucs)]\n",
    "    \n",
    "    # create summary DataFrame for all score types\n",
    "    score_types = [\"accuracy\", \"precision\", \"recall\", \"F1\", \"specificity\", \"AUC\"]\n",
    "    summary_df = pd.DataFrame({\"mean\": mean_summary, \"STD\": std_summary}, index=score_types)\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanityCheck4(predictions):\n",
    "    \n",
    "    ones_ratio = np.mean(predictions)\n",
    "    zeros_ratio = 1 - ones_ratio\n",
    "    \n",
    "    print(\"'1' predictions: around \"+str(int(ones_ratio*100))+\"%\")\n",
    "    print(\"'0' predictions: around \"+str(int(zeros_ratio*100))+\"%\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelect(processed_X, processed_bi_y, n_features):\n",
    "    \n",
    "    # create and fit the feature selector\n",
    "    svm = SVC(kernel='rbf', C=1, gamma=1)\n",
    "    sfs = SequentialFeatureSelector(svm, n_features_to_select = n_features)\n",
    "    sfs.fit(processed_X, processed_bi_y)\n",
    "\n",
    "    # get the selected features\n",
    "    selection_mask = sfs.get_support()\n",
    "    feature_selection = np.array(feature_list)[selection_mask]\n",
    "    \n",
    "    return feature_selection, selection_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateFeatureSet(processed_X, processed_bi_y, selection_mask):\n",
    "    \n",
    "    # retain the selected features only\n",
    "    new_feature_X = processed_X[:,selection_mask]\n",
    "    \n",
    "    # evalusate the model with these features\n",
    "    summary_df, predictions = svmModel(new_feature_X, processed_bi_y, SVC(kernel='rbf', C=1, gamma=1))\n",
    "    \n",
    "    return summary_df, predictions, new_feature_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearch(final_X, processed_bi_y):\n",
    "    \n",
    "    # parameters to grid search\n",
    "    param_grid = {'C': [0.01, 0.05, 0.1, 1, 10], 'gamma': [0.001, 0.01, 0.05, 0.1, 1, 10], 'kernel': ['rbf']}\n",
    "    \n",
    "    # score variables\n",
    "    best_accuracy=0\n",
    "    best_auc=0\n",
    "    \n",
    "    # iterate over the parameter grid\n",
    "    for g in ParameterGrid(param_grid):\n",
    "\n",
    "        # cross-validate the model and record its performance\n",
    "        current_model_scores, _ = svmModel(final_X, processed_bi_y, SVC(**g))\n",
    "        current_accuracy = current_model_scores.to_dict()[\"mean\"][\"accuracy\"]\n",
    "        current_auc = current_model_scores.to_dict()[\"mean\"][\"AUC\"]\n",
    "        \n",
    "        # save a best accuracy score\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            best_parameters_accuracy = g\n",
    "            \n",
    "         # save a best auc score\n",
    "        if current_auc > best_auc:\n",
    "            best_auc = current_auc\n",
    "            best_parameters_auc = g\n",
    "            \n",
    "    # report scores to user\n",
    "    print(f'Best accuracy is {best_accuracy} for {best_parameters_accuracy} SVM parameters.')\n",
    "    print(f'Best AUC is {best_auc} for {best_parameters_auc} SVM parameters.')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutationTest(n_permutations, X, y, model):\n",
    "    \n",
    "    # initializations\n",
    "    all_permutations = dict()\n",
    "    score_types = [\"accuracy\", \"precision\", \"recall\", \"F1\", \"specificity\", \"AUC\"]\n",
    "    \n",
    "    # calculate the model scores\n",
    "    for score_type in score_types:\n",
    "        score_df, _ = svmModel(X, y, model)\n",
    "        true_score = score_df.to_dict()[\"mean\"][score_type]\n",
    "        all_permutations[score_type] = {\"perm_scores\": [], \"true_score\": true_score}\n",
    "    \n",
    "    # calculate all score types for all permutations\n",
    "    for i in range(n_permutations):\n",
    "        \n",
    "        y_ = np.random.permutation(y)\n",
    "        cur_prem_score_df, _ = svmModel(X, y_, model)\n",
    "        cur_perm_scores_summary = cur_prem_score_df.to_dict()\n",
    "        \n",
    "        for score_type in score_types:          \n",
    "            cur_perm_score = cur_perm_scores_summary[\"mean\"][score_type]\n",
    "            all_permutations[score_type][\"perm_scores\"].append(cur_perm_score)\n",
    "        \n",
    "    #calculate p values for all scores\n",
    "    p_values = np.array([])\n",
    "    \n",
    "    for score_type in score_types:\n",
    "        all_perm_scores = np.array(all_permutations[score_type][\"perm_scores\"])\n",
    "        true_score = all_permutations[score_type][\"true_score\"]\n",
    "        higher_then_true_scores_count = (all_perm_scores >= true_score).sum()\n",
    "        p_value = (higher_then_true_scores_count+1)/(n_permutations+1)\n",
    "        p_values = np.append(p_values, p_value)\n",
    "    \n",
    "    # create the evaluation dataframe\n",
    "    summary_df = pd.DataFrame({\"p value\": p_values}, index=score_types)\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalModel(final_X, processed_bi_y, new_C, new_gamma):\n",
    "    \n",
    "    svm_model = SVC(kernel='rbf', C=new_C, gamma=new_gamma)\n",
    "    svm_model.fit(final_X, processed_bi_y)\n",
    "    \n",
    "    return svm_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
